# EM Algorithm
## K-means Clustering
K-means clustering is a method that aims to partition $n$ observations into $k$ clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.

Given a training set $\{ x^{(1)}, \ldots, x^{(m)} \}$ and no labels $y^{(i}$, 

1. Initialize cluster centroids $\mu_1, \mu_2, \ldots, \mu_k \in \mathbb{R}^n$ randomly.

2. Repeat until convergence.  

For every $i$,
$$
c^{(i)} := \arg\min_j \|x^{(i)} - \mu_j\|^2.
$$
For each $j$,
$$
\mu_j := \frac{\sum_{i=1}^m 1\{c^{(i)} = j\} x^{(i)}}{\sum_{i=1}^m 1\{c^{(i)} = j\}}.
$$
Where , $k$ (a parameter of the algorithm) is the number of clusters we want to find, and the cluster centroids $\mu_j$ represent our current guesses for the positions of the centers of the clusters.

Mathmatically, partition the $n$ observations into $k (≤ n)$ sets $S = \{S_1, S_2, \cdots, S_k\}$ so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). 
$$
\arg\min_{\mathcal{S}} \sum_{i=1}^k \sum_{\mathbf{x} \in S_i} \|\mathbf{x} - \mu_i\|^2 = \arg\min_{\mathcal{S}} \sum_{i=1}^k |S_i| \operatorname{Var}(S_i)
$$
Where mean(cluster centroid) of points in $S_i$ is, 
$$
\mu_i = \frac{1}{|S_i|} \sum_{\mathbf{x} \in S_i} \mathbf{x},
$$

## Density Estimation
Density Estimation is the construction of an estimate of an unobservable underlying probability density function, based on observed data. A very natural use of density estimates is in the informal investigation of the properties of a given set of data.  
Therefore Density estimation is also frequently used in anomaly detection or novelty detection. If an observation lies in a very low-density region, it is likely to be an anomaly or a novelty.  
We can also assume that the observed data points of Density Estimation are distributed from multiple mixture of Gaussian distributions.

### Problem of Density Estimation
However the problem of Density Estimation is that you can only see the data came from set of Gaussains, but you don't know which example came from which Gaussian.  
Therefore Expectation-Maximization algorithm will allow us to fit the model despite not knowing which Gaussian each example that came from.

## Mixture of Gaussians
When Mixture of Gaussian is a function that is composed of $k$ number of gaussian, each Gaussian is comprised of 3 parameters.
 - mean $\mu$ that defines its center.
 - covariance $\Sigma$ that defines its width. This would be equivalent to the dimensions of an ellipsoid in a multivariate scenario.
 - mixing probability $\phi$ represents the weights of each Gaussian component in the mixture of Gaussians.
It is a method used to determine the probability each data point belongs to a given cluster.

In MoG Model, each $x^{(i)}$ from training set was generated by randomly choosing $z^{(i)}$ $\{1, \cdots, k\}$, and then $x^{(i)}$ was drawn from one of $k$ Gaussians depending on $z^{(i)}$. When $z^{(i)}$'s are latent random variables, meaning that they’re hidden/unobserved.
$$
\sum_{i=1}^{m} p(x^{(i)}; \phi, \mu, \Sigma) = \sum_{i=1}^{m} \sum_{z^{(i)}=1}^{k} p(x^{(i)} \mid z^{(i)}; \mu, \Sigma) p(z^{(i)}; \phi)
$$

 - $ p(x^{(i)}, z^{(i)}) = p(x^{(i)} \mid z^{(i)})p(z^{(i)})$ 
   - indicates the probability of observing $x^{(i)}$ along with its corresponding latent variable $Z^{(i)}$, and applying Bayes' rule.  
 - $\phi_j = p(z^{(i)} = j)$
   - indicates that $\phi_j$ is the probability that the $z^{(i)}$ belongs to the $j$-th cluster.
 - $z^{(i)} \sim \text{Multinomial}(\phi), \quad (\phi_j \geq 0, \quad \sum_{j=1}^{k} \phi_j = 1)$
   - indicates that e latent variable $z^{(i)}$, which represents the cluster assignment for $x^{(i)}$ is drawn from a multinomial distribution. In other words, $z^{(i)}$ is drawn from the multinomial distribution with probabilities $\phi$.  
   - For example, when $k=3$ and $\phi = (0.2,0.5,0.3)$, $P(z^{(i)} = 1) = 0.2, \quad P(z^{(i)} = 2) = 0.5, \quad P(z^{(i)} = 3) = 0.3.$
 - $x^{(i)} \mid z^{(i)} = j \sim \mathcal{N}(\mu_j, \Sigma_j)$
   - indicates that data point $x^{(i)}$ is generated by sampling from the Gaussian distribution with mean $\mu_j$ and covariance $\Sigma_j$ but only if the latent variable $z^{(i)}$ assigns it to cluster $j$.

### Multivariate Gaussian, Mixture of Gaussian and Gaussian Discriminant Analysis(GDA)
#### GDA vs Mixture of Gaussian
 - In GDA, we have labled examples $(x_i, y_i)$ when $y_i$ is obseved. In Mixture of Gaussian, $y_i$ is replaced with $z_i$ which is latent(=hidden) random variable that we can not observer in the training set.
 - Mixture of Gaussian set $z(y)$ to one of $k$ values instead of two in GDA. In GDA, $y(z)$ takes on one of two values. 
 - Mixture of Gaussian use $\Sigma_{j}$ instead of $\Sigma$ in GDA, which means each Gaussian model in Mixture of Gaussian uses its own covariance matrix.

#### Multivariate Gaussian vs Mixture of Gaussian
While $k$ dimensional Multivariate Gaussian is, 
$$
\mathbf{X} = \begin{pmatrix} X_1, \dots, X_k \end{pmatrix} \sim \text{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}),
$$
Where $\mu = \{\mu_1, \mu_2, \cdots, \mu_k \}$ is the mean vector, and $\Sigma$ is the positive definite $k \times k$ covariance matrix.

However, we can write out a $k$ component mixture of (1 dimensional) Gaussians as, 
$$
\mathbf{X} \sim \sum_{i=1}^{k} \pi_i \, \text{N}(\mu_i, \Sigma_i),
$$
where $\pi_i$ is the mixing proportion of the $i$-th component and $(\mu_i, \Sigma_i)$ are the parameters of the $i$-th component.  
As above, the Multivariate Gaussian is defined as a $k$ dimensional random vector, and the Mixture of Gaussians is defined as a random variable (which you can call a $1$ dimensional random vector).  

In conclusion, 
 - A Multivariate Gaussian Distribution describes a single, unified probability density over a vector of random variables, capturing the relationships (covariances) between them.  
 - A Mixture of Gaussians is a probabilistic model that represents a weighted sum of multiple Gaussian distributions. It is designed to model data that comes from several distinct subpopulations, each of which follows its own Gaussian distribution.

## Jensen's Inequality
Jensen's inequality generalizes the statement that the secant line(a line that intersects a curve at a minimum of two distinct points) of a convex function lies above the graph of the function. In the context of probability theory, it is generally stated in the following form:  
If $X$ is a random variable and $\varphi$ is a convex function, then
$$
\varphi(\mathbb{E}[X]) \leq \mathbb{E}[\varphi(X)]
$$
If $\varphi$ is concave function, then
$$
\varphi(\mathbb{E}[X]) \geq \mathbb{E}[\varphi(X)]
$$
The difference between the two sides of the inequality is called the Jensen gap.
$$
\varphi(\mathbb{E}[X]) - \mathbb{E}[\varphi(X)]
$$
Moreover, if function $\varphi$ is strictly convex(or concave), 
$$
\varphi(\mathbb{E}[X]) = \mathbb{E}[\varphi(X)]
$$
Above is true if and only if $X=E[X]$ with probability 1.  (i.e., if $X$ is a constant)

## Expectation–Maximization Algorithm (EM Algorithm)
An expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved(latent) variables.  
EM Algorithm chooses some random values for the latent data points and estimates a new set of data. These new values are then recursively used to estimate a better first date, by filling up unknown points, until the values get fixed.  
The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step.   
These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.  
It can be used, for example, to estimate a mixture of gaussians.

### Description of EM Algorithm
Suppose we have an estimation problem in which we have a training set $\{x^{(1)}, \cdots, x^{(m)} \}$ consisting of $m$ independent examples. We wish to fit the parameters of a model $p(x, z)$ to the data where the likelihood is given as below.
$$
\ell(\theta) = \sum_{i=1}^m \log p(x; \theta) = \sum_{i=1}^m \log \sum_{z} p(x, z; \theta).
$$
Since $z^{(i)}$'s are latent random variables, instead of observed data points, we can't use maximum likelihood estimation. 

EM algorithm gives an efficient method for maximum likelihood estimation.  
Since Maximizing $\ell(\theta)$ explicitly is difficult(MLE), instead,
 - E-step: we will repeatedly construct a lower-bound on $\ell(\theta)$. 
 - M-step: and then optimize that lower-bound.
![alt text](images/blog25_em_algorithm.png)

### Expectation Step (E Step)
For each $i$, let $Q_i$ be some distribution over the discrete variables $z$’s Where $\sum Q_i(z^{(i)}) = 1, \quad Q_i(z^{(i)}) \geq 0$, following equations can be derived.
$$
\ell(\theta) = \sum_{i=1}^m \log p(x; \theta) = \sum_i \log p(x^{(i)}; \theta)  
= \sum_i \log \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta) \\
= \sum_i \log \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} \\ 
\geq \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}
$$
For the understading of the derivation above, consider following as an expectation of the quantity $$ with respect to $z^{(i)}$ from distribution $Q_i$.  
(For example, when $z = \{1, 2, \cdots, 10 \}$, $\mathbb{E}[g(z)] = \sum_z p(z)g(z)$ and $\mathbb{E} = \sum_z p(z) z$.)
$$
\sum_{z^{(i)}} Q_i(z^{(i)}) \left[ \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} \right]
$$
From this idea, we apply Jensen's inequality and we have as below.
$$
f \left( \mathbb{E}_{z^{(i)} \sim Q_i} \left[ \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} \right] \right) 
\geq 
\mathbb{E}_{z^{(i)} \sim Q_i} \left[ f \left( \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} \right) \right],
$$
So this makes the previous two equations works.
$$
\sum_i \log \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}  
\geq \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}
$$
Now, for any set of distributions $Q_i$, We have a lower-bound on $\ell(\theta)$.  
After that, it is natural to try to make the lower-bound tight at that value of $\theta$.  
In other word, make the inequality above hold with equality at our particular value of $\theta$ as following.
$$
\sum_i \log \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}  
= \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}
$$
To make above true, it is sufficent that the expectation be taken over a constant value. 
$$
\frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})} = c
$$
This is because,
$$
f \left( \mathbb{E}_{z^{(i)} \sim Q_i} \left[ c \right] \right) = 
f \left( \mathbb{E} \left[ c \right] \right) =
f(c)
\\ 
\mathbb{E}_{z^{(i)} \sim Q_i} \left[ f \left( c \right) \right] = 
\mathbb{E} \left[ f \left( c \right) \right] = 
f(c)
$$
This can be accomplished by setting $Q_i(z^{(i)})$ is proportion of $p(x^{(i)}, z^{(i)})$.
$$
Q_i(z^{(i)}) \propto p(x^{(i)}, z^{(i)}; \theta).
$$
To make the $Q_i$ proportion of $p(x^{(i)}, z^{(i)})$, we will use the fact that we know, which is $\sum_{z^{(i)}} Q_i(z^{(i)}) = 1$.
$$
Q_i(z^{(i)}) = \frac{p(x^{(i)}, z^{(i)}; \theta)}{\sum_z p(x^{(i)}, z; \theta)}
= \frac{p(x^{(i)}, z^{(i)}; \theta)}{p(x^{(i)}; \theta)}
= p(z^{(i)} \mid x^{(i)}; \theta).
$$
So by simply set the $Q_i$’s to be the posterior distribution of the $z^{(i)}$’s given $x^{(i)}$
and the setting of the parameters $\theta$, we can make the Jensen’s inequality in our derivation hold with equality.  

At last, We have a lower-bound on the loglikelihood $\ell$ that we’re trying to maximize.

### Maximization Step (M Step)
Maximize the following formula that we derived from E Step which indicates the lower bound of the current log likelihood with respect to the parameters, so that we can obtain a new setting of the $\theta$.
$$
\sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}
$$
(Detailed calculation will be explained in the "Applying EM Algorithm to Mixture of Gaussians" using Gaussian Mixtures Model example.)

So combining both E Step and M step,  

$$
\textbf{Repeat until convergence} \\ 
\text{(E-step)} \quad \text{For each } i, \text{ set} \quad Q_i(z^{(i)}) := p(z^{(i)} \mid x^{(i)}; \theta). \\
\text{(M-step)} \quad \text{Set} \quad \theta := \arg \max_\theta \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}. \\
$$

### Convergence of EM
How we we know if this algorithm will converge? We will prove that 
$$\ell(\theta^{(t)}) \leq \ell(\theta^{(t+1)})$$
So that EM always monotonically improves log likelihood.  
We will use the result of choosing $Q_i$'s from derivation above, remind that,
$$
Q_i^{(t)}(z^{(i)}) = p(z^{(i)} \mid x^{(i)}; \theta^{(t)}) \\
\ell(\theta^{(t)}) = \sum_i \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta^{(t)})}{Q_i^{(t)} (z^{(i)})}
$$
with difference of $t$ which indicates the iteration.  
$$
\ell(\theta) \geq \sum_i \sum_{z^{(i)}} Q_i (z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta}{Q_i (z^{(i)})} 
$$
Since above equation is true for any values of $Q_i$'s and $\theta$, particularly it holds also true for  $Q_i = Q_i^{(t)}$ and $\theta = \theta^{(t+1)}$ as below.
$$
\ell(\theta^{(t+1)})
\geq \sum_i \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta^{(t+1)})}{Q_i^{(t)} (z^{(i)})}
$$
Since $\theta^{(t+1)}$ is explicitly is chosen from following equation, 
$$
\theta^{(t+1)} = \arg\max_{\theta} \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})},
$$
We can assure that $\theta^{(t+1)}$ must be equal or larger than the same formula evaluated at  $\theta^{(t)}$.
$$
\sum_i \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta^{(t+1)})}{Q_i^{(t)} (z^{(i)})}
\geq \sum_i \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta^{(t)})}{Q_i^{(t)} (z^{(i)})} = \ell(\theta^{(t)})
$$
Therefore combining all together, we can find out that EM causes the likelihood to converge monotonically.
$$
\ell(\theta^{(t+1)})
\geq \sum_i \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta^{(t+1)})}{Q_i^{(t)} (z^{(i)})} 
\geq \sum_i \sum_{z^{(i)}} Q_i^{(t)} (z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta^{(t)})}{Q_i^{(t)} (z^{(i)})} = \ell(\theta^{(t)})
$$

### Coordinate Ascent aspect of EM Algorithm
Coordinate descent is an optimization algorithm that successively minimizes along coordinate directions to find the minimum of a function. 
At each iteration, the algorithm determines a coordinate via a coordinate selection rule, then minimizes over the corresponding coordinate hyperplane while fixing all other coordinates or coordinate block.
$$
J(Q, \theta) = \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) \log \frac{p(x^{(i)}, z^{(i)}; \theta)}{Q_i(z^{(i)})}
$$
From the view of definition of coordinate ascent, EM can also be viewed a coordinate ascent on $J$, in which the E-step maximizes it with
respect to $Q$(choosing $Q_i$ in E step) and the M-step maximizes it with respect to $\theta$.

### Applying EM Algorithm to Mixture of Gaussians
Armed with our general definition of the EM algorithm, let’s go back to our
old example of fitting the parameters $\phi, \mu, \Sigma$ in a mixture of Gaussians.

#### Estimation Step (E): Tries to “guess” the latent values of the $z^{(i)}$
First, initialize our model parameters like the mean($\mu_j$), covariance matrix($\Sigma_j$), and mixing coefficients($\phi_j$).  
Calculate the posterior probabilities of data points belonging to each centroid using the current parameter values. In other words, using current paratmers values(mean, covariance and mixing probability), calculate the posterior probability of $z^{(i)}$'s given $x^{(i)}$'s.

$$
\textbf{(For each $i$,$j$)} \quad w_j^{(i)} := p(z^{(i)} = j \mid x^{(i)}; \phi, \mu, \Sigma)
$$

The values $w^{(i)}_j$ calculated in the E-step abvove is soft guesses for the $z^{(i)}$, whici is the probability of how much $x^{(i)}$ is assigned to the $j$ Gaussian.

#### Maximazation Step (M): Update parameter values($\phi, \mu, \Sigma$)
In M Step, pretending that the guesses in the E step were correct, updates the parameters of the model based on the guesses.  

$$
\phi_j := \frac{1}{m} \sum_{i=1}^m w_j^{(i)}, \\

\mu_j := \frac{\sum_{i=1}^m w_j^{(i)} x^{(i)}}{\sum_{i=1}^m w_j^{(i)}}, \\

\Sigma_j := \frac{\sum_{i=1}^m w_j^{(i)} \left( x^{(i)} - \mu_j \right) \left( x^{(i)} - \mu_j \right)^T}{\sum_{i=1}^m w_j^{(i)}}.
$$


## Factor Analysis
Factor analysis is a statistical method used to describe variance among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  
For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables.  
Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus "error" terms, hence factor analysis can be thought of as a special case of errors-in-variables models.
In perspective of Machine Learning, Factor analysis is one of the unsupervised machine learning algorithms which is used for dimensionality reduction. This algorithm creates factors from the observed variables to represent the common variance(i.e. variance due to correlation among the observed variables).

### Why Use Factor Analysis
When $n$ is dimension of data from a mixture of several Gaussians and $m$ is the number of training set, if $n >> m$, we would find that the covariance matrix $\Sigma$  is singular.  
This means that $(1/|\Sigma|^{1/2}) = \frac{1}{0}$ does not exist, and $\frac{1}/\Sigma1/2 = 1/0$.

More generally, unless $m$ exceeds $n$ by some reasonable amount, the maximum likelihood estimates of the mean and covariance may be quite poor.  
Nonetheless, we would still like to be able to fit a reasonable Gaussian model to the data, and perhaps capture some interesting covariance structure in the data. We can do it by using Factor Analysis.

### Factor Analysis Model

### EM for Factor Analysis