# Independent Components Analysis (ICA)
## Preliminaries
### CDF and PDF
### Density and Probability
### Permutation Matrix
A permutation matrix is a square binary matrix that has exactly one entry of 1 in each row and each column with all other entries 0.
For example,
$$
P =
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{bmatrix},
\quad
v =
\begin{bmatrix}
v_1 \\ v_2 \\ v_3
\end{bmatrix}
$$
$Pv$ is as following.
$$
Pv =
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
v_1 \\ v_2 \\ v_3
\end{bmatrix}
=
\begin{bmatrix}
v_2 \\ v_3 \\ v_1
\end{bmatrix}
$$
#### Property of Permutation Matrix
- The inverse of a permutation matrix($P^{-1}$) is also a permutation matrix.
- The inverse is simply the transpose: $P^T = P^{-1}$

This holds for all permutation matrices, regardless of size.

### Rotatinal Matrix
### Rotational Symmetric


## Basics
Independent Component Analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. It aims to find a linear transformation of data that maximizes statistical independence among the components.  
ICA is widely applied in fields like audio, image processing, and biomedical signal analysis to isolate distinct sources from mixed signals.

### Cocktail Party Problem


## Definition of ICA
Assume that we observe $n$ linear mixtures $x_1, 
\cdots ,x_n$ of $n$ independent components. Since independent component $s_k$ is random variable, linear transformation of $s$ is $x$ and therefore $x$ is also random variable. But $x$ is just a mixture of independent components($=s$), but $x$ itself is not independent.
$$
x_j = \sum_{j}^m a_{j1}s_1 + a_{j2}s_2 + ... + a_{jn}s_n
$$
It is convenient to use vector-matrix notation instead of the sums like in the previous equation.  
Let us denote by $x$ the random vector whose elements are the mixtures $x_1, \cdots x_n$, and likewise by $s$ the random vector with elements $s_1, \cdots , s_n$. Let us denote by $A$ the matrix with elements $a_{ij}$. 

$$ x = As $$

The ICA model is a generative model, which means that it describes how the observed data are generated by a process of mixing the components $s_k$ when some data $s \in \mathbb{R}^n$ that is generated via $n$ independent sources.

So $s_k$'s are latent variables, meaning that they cannot be directly observed. Also the mixing matrix $A$ is assumed to be unknown.  
All we observe is the random vector $x$, and we must estimate both $A$ and $s$ using it.
There is  What we observe is $x$, where $A$ is an unknown square matrix called the mixing matrix.  

Therefore, our goal is to recover the sources $s$ that had generated our data. In other words, after estimating the matrix $A$, we can compute its inverse, say $W$($W = A^{-1}$), and obtain the independent component simply by:
$$
{\bf s}={\bf W}{\bf x}.
$$

## Ambiguities of ICA
It is easy to see that the following ambiguities will hold.

1. We cannot determine the correct scaler of the sources($A, s$).  
The reason is that, both $s$ and $A$ being unknown, any scalar multiplier in one of the sources $s_k$ could always be cancelled by dividing the corresponding column $a_j$ of by the same scalar.  
For example, if $A$ were replaced with $2A$ and every $s_k$ were replaced with $(0.5)s_k$,observed $x_k=2AÂ·(0.5)s_k$ would still be the same.

2. We cannot determine the order of the sources($A, s$).  
The reason is that, again both $s$ and $A$ being unknown, we can freely change the order of the terms in the sum.  
For example, below is the case when $S_{\text{estimated}}$ is correctly recovered.
$$
X = AS =
\begin{bmatrix} 
2 & 1 \\ 
1 & 3 
\end{bmatrix}
\begin{bmatrix} 
1 & 2 \\ 
3 & 4 
\end{bmatrix}  =
\begin{bmatrix}
5 & 8 \\
10 & 14
\end{bmatrix}
$$
$$
W = A^{-1} =
\begin{bmatrix}
2 & 1 \\
1 & 3
\end{bmatrix}^{-1}
=
\begin{bmatrix}
\frac{3}{5} & -\frac{1}{5} \\
-\frac{1}{5} & \frac{2}{5}
\end{bmatrix}
$$
$$
S_{\text{estimated}} = WX = 
\begin{bmatrix}
\frac{3}{5} & -\frac{1}{5} \\
-\frac{1}{5} & \frac{2}{5}
\end{bmatrix}
\begin{bmatrix}
5 & 8 \\
10 & 14
\end{bmatrix}
=
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
$$
However, with permutation matrix $P = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$, 
$$
\tilde{W} = PW
$$
$$
\tilde{W} =
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
\frac{3}{5} & -\frac{1}{5} \\
-\frac{1}{5} & \frac{2}{5}
\end{bmatrix}
=
\begin{bmatrix}
-\frac{1}{5} & \frac{2}{5} \\
\frac{3}{5} & -\frac{1}{5}
\end{bmatrix}
$$
$$
S_{\text{estimated, permuted}} = \tilde{W} X = 
\begin{bmatrix}
-\frac{1}{5} & \frac{2}{5} \\
\frac{3}{5} & -\frac{1}{5}
\end{bmatrix}
\begin{bmatrix}
5 & 8 \\
10 & 14
\end{bmatrix}
=
\begin{bmatrix}
3 & 4 \\
1 & 2
\end{bmatrix}
$$
The estimated sources are permuted but still correct in terms of independence. (column space = span remains unchanged )

## Algorithm