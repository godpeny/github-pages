# Independent Components Analysis (ICA)
## Preliminaries
### CDF and PDF
### Density and Probability
### Permutation Matrix
A permutation matrix is a square binary matrix that has exactly one entry of 1 in each row and each column with all other entries 0.
For example,
$$
P =
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{bmatrix},
\quad
v =
\begin{bmatrix}
v_1 \\ v_2 \\ v_3
\end{bmatrix}
$$
$Pv$ is as following.
$$
Pv =
\begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
v_1 \\ v_2 \\ v_3
\end{bmatrix}
=
\begin{bmatrix}
v_2 \\ v_3 \\ v_1
\end{bmatrix}
$$
#### Property of Permutation Matrix
- The inverse of a permutation matrix($P^{-1}$) is also a permutation matrix.
- The inverse is simply the transpose: $P^T = P^{-1}$

This holds for all permutation matrices, regardless of size.

### Rotatinal Matrix
### Rotational Symmetric


## Basics
Independent Component Analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. It aims to find a linear transformation of data that maximizes statistical independence among the components.  
ICA is widely applied in fields like audio, image processing, and biomedical signal analysis to isolate distinct sources from mixed signals.

### Cocktail Party Problem


## Definition of ICA
Assume that we observe $n$ linear mixtures $x_1, 
\cdots ,x_n$ of $n$ independent components. Since independent component $s_k$ is random variable, linear transformation of $s$ is $x$ and therefore $x$ is also random variable. But $x$ is just a mixture of independent components($=s$), but $x$ itself is not independent.
$$
x_j = \sum_{j}^m a_{j1}s_1 + a_{j2}s_2 + ... + a_{jn}s_n
$$
It is convenient to use vector-matrix notation instead of the sums like in the previous equation.  
Let us denote by $x$ the random vector whose elements are the mixtures $x_1, \cdots x_n$, and likewise by $s$ the random vector with elements $s_1, \cdots , s_n$. Let us denote by $A$ the matrix with elements $a_{ij}$. 

$$ x = As $$

The ICA model is a generative model, which means that it describes how the observed data are generated by a process of mixing the components $s_k$ when some data $s \in \mathbb{R}^n$ that is generated via $n$ independent sources.

So $s_k$'s are latent variables, meaning that they cannot be directly observed. Also the mixing matrix $A$ is assumed to be unknown.  
All we observe is the random vector $x$, and we must estimate both $A$ and $s$ using it.
There is  What we observe is $x$, where $A$ is an unknown square matrix called the mixing matrix.  

Therefore, our goal is to recover the sources $s$ that had generated our data. In other words, after estimating the matrix $A$, we can compute its inverse, say $W$($W = A^{-1}$), and obtain the independent component simply by:
$$
{\bf s}={\bf W}{\bf x}.
$$

## Ambiguities of ICA
It is easy to see that the following ambiguities will hold.

### Scalar of the Sources($A, s$)  
We cannot determine the correct scaler of the sources($A, s$).  
The reason is that, both $s$ and $A$ being unknown, any scalar multiplier in one of the sources $s_k$ could always be cancelled by dividing the corresponding column $a_j$ of by the same scalar.  
For example, if $A$ were replaced with $2A$ and every $s_k$ were replaced with $(0.5)s_k$,observed $x_k=2AÂ·(0.5)s_k$ would still be the same.

### Order of the Sources($A, s$)  
We cannot determine the order of the sources($A, s$).  
The reason is that, again both $s$ and $A$ being unknown, we can freely change the order of the terms in the sum.  
For example, below is the case when $S_{\text{estimated}}$ is correctly recovered.
$$
X = AS =
\begin{bmatrix} 
2 & 1 \\ 
1 & 3 
\end{bmatrix}
\begin{bmatrix} 
1 & 2 \\ 
3 & 4 
\end{bmatrix}  =
\begin{bmatrix}
5 & 8 \\
10 & 14
\end{bmatrix}
$$
$$
W = A^{-1} =
\begin{bmatrix}
2 & 1 \\
1 & 3
\end{bmatrix}^{-1}
=
\begin{bmatrix}
\frac{3}{5} & -\frac{1}{5} \\
-\frac{1}{5} & \frac{2}{5}
\end{bmatrix}
$$
$$
S_{\text{estimated}} = WX = 
\begin{bmatrix}
\frac{3}{5} & -\frac{1}{5} \\
-\frac{1}{5} & \frac{2}{5}
\end{bmatrix}
\begin{bmatrix}
5 & 8 \\
10 & 14
\end{bmatrix}
=
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
$$
However, with permutation matrix $P = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$, 
$$
\tilde{W} = PW
$$
$$
\tilde{W} =
\begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
\frac{3}{5} & -\frac{1}{5} \\
-\frac{1}{5} & \frac{2}{5}
\end{bmatrix}
=
\begin{bmatrix}
-\frac{1}{5} & \frac{2}{5} \\
\frac{3}{5} & -\frac{1}{5}
\end{bmatrix}
$$
$$
S_{\text{estimated, permuted}} = \tilde{W} X = 
\begin{bmatrix}
-\frac{1}{5} & \frac{2}{5} \\
\frac{3}{5} & -\frac{1}{5}
\end{bmatrix}
\begin{bmatrix}
5 & 8 \\
10 & 14
\end{bmatrix}
=
\begin{bmatrix}
3 & 4 \\
1 & 2
\end{bmatrix}
$$
The estimated sources are permuted but still correct in terms of independence. (column space = span remains unchanged)

### Why $s$ has to be Non-Gaussian?
It turns out that these two are the only ambuguities so long as the sources $s$ are non-Gaussian.   
Let's see what the difficulty is with Gaussian data. But before move on, you should know several notes as below.

 - Note 1: In ICA, the source signals $s$ are typically assumed to have zero mean and unit variance (variance = 1). 
 - Note 2: Also Note that the contours of the density of the standard normal distribution $N(0,I)$ are circles centered on the origin, and the density is rotationally symmetric. 
 - Note 3: A multivariate Gaussian distribution(which is $s$ in this case) has a spherical symmetry in high dimensions. This means that any orthogonal rotation of $s$ will still be Gaussian.

Using above notes, let $s \sim \text{N}(0,I)$ is independent variable with zero mean and unit variance. Consier observed value $x = As$,
$$
\mathbb{E}[xx^T] = \mathbb{E}[A s s^T A^T] = A A^T.
$$ 
Where $A$ is our mixing matrix.
Above equation can be derived because covariance of $s$ can be calculated as, 
$$
\text{Cov}(s) = \mathbb{E} \left[ (s - \mathbb{E}[s])(s - \mathbb{E}[s])^T \right] = \mathbb{E}[s s^T]
$$
Since $\mathbb{E}[s] = 0$ from given condition.

Now Let's think of rotational matrix $R$ and apply to mixing matrix $A$. When $A' = AR$, Then if the data $x$ had been mixed according to $A$ would have instead observed as $x' = A's$.  
The distribution of $x'$ is also Gaussian, with zero mean and covariance $AA^T$.  
This is because mean can be calcuated as 
$$\mathbb{E}[x'] = A \mathbb{E}[s]$$ 
When $\mathbb{E}[s] = 0$.  

Also variance is 
$$\mathbb{E}[x' (x')^T] = \mathbb{E}[A' s s^T (A')^T] = \mathbb{E}[A R S S^T (A R)^T] = A R R^T A^T = A A^T
$$

Therefore, whether the mixing matrix is $A$ or $A'$, we would observe data from $x \sim \text{N}(0,AA^T)$ distribution.  
So, there is no way to tell if the sources were mixed using $A$ and $A'$.
If there is an arbitrary rotational component $R$ in the mixing matrix that cannot be determined from the data, we cannot recover the original sources.  
In other words, as long as the data is not Gaussian, it is possible to recover the $n$ independent sources.

## Algorithm