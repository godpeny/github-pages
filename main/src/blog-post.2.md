# Softmax and Cross Entropy

## Basic

## Softmax

## Gradient of Softmax

## Cross Entropy
https://en.wikipedia.org/wiki/Cross-entropy

### Cross Entropy Loss vs Negative Log Likelihood
Cross entropy is negative log likelihood.  
It is because minimizing cross entropy is same as maximizing  log likelihood.

###  Cross Entropy Loss
https://www.geeksforgeeks.org/what-is-cross-entropy-loss-function/
https://stackoverflow.com/questions/41990250/what-is-cross-entropy


## Gradient of Cross Entropy